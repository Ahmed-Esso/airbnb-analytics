{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606fe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_14884\\3575948579.py:4: DtypeWarning: Columns (24,25,26,27,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"all_cities.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Convert cleanliness rating from 10-scale to 5-scale\n",
    "df['cleanliness_rating'] = df['cleanliness_rating'] / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c023e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.rename(columns={'lat': 'latitude', 'lng': 'longitude'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path2 = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"scraped_data.csv\")\n",
    "df2 = pd.read_csv(path2)\n",
    "\n",
    "df2.rename(columns={'price': 'realSum'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c95051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create room type columns\n",
    "df2['room_shared'] = df2['room_type'].apply(lambda x: True if x == 'Shared room' else False)\n",
    "df2['room_private'] = df2['room_type'].apply(lambda x: True if x == 'Private room' else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0b1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename accommodates to person_capacity\n",
    "df2.rename(columns={'accommodates': 'person_capacity'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f9e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process host_is_superhost column\n",
    "df2.dropna(subset=['host_is_superhost'], inplace=True)\n",
    "\n",
    "mapping = {'t': 1, 'f': 0, True: 1, False: 0, 'True': 1, 'False': 0}\n",
    "df2['host_is_superhost'] = df2['host_is_superhost'].map(mapping)\n",
    "\n",
    "df2.dropna(subset=['host_is_superhost'], inplace=True)\n",
    "df2['host_is_superhost'] = df2['host_is_superhost'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19149dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi and biz columns based on host listings count\n",
    "df2['multi'] = df2['host_listings_count'].apply(lambda x: 1 if x > 1 else 0)\n",
    "df2['biz'] = df2['host_listings_count'].apply(lambda x: 1 if x > 1 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f21268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process cleanliness rating\n",
    "df2.rename(columns={'review_scores_cleanliness': 'cleanliness_rating'}, inplace=True)\n",
    "df2['cleanliness_rating'] = df2['cleanliness_rating'].round(1)\n",
    "df2['cleanliness_rating'] = df2['cleanliness_rating'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c4876cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process guest satisfaction overall\n",
    "df2.rename(columns={'host_acceptance_rate': 'guest_satisfaction_overall'}, inplace=True)\n",
    "df2.dropna(subset=['guest_satisfaction_overall'], inplace=True)\n",
    "df2['guest_satisfaction_overall'] = df2['guest_satisfaction_overall'].str.replace('%', '', regex=False).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f83a6e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create empty columns for distance and index features\n",
    "new_columns = ['dist', 'metro_dist', 'attr_index', 'attr_index_norm', 'rest_index', 'rest_index_norm']\n",
    "for col in new_columns:\n",
    "    df2[col] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9e926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for allowed cities\n",
    "allowed_cities = ['amsterdam', 'athens', 'barcelona', 'berlin', 'budapest', 'vienna', 'lisbon', 'london', 'paris', 'rome']\n",
    "\n",
    "location_split = df2['host_location'].str.split(',', n=1, expand=True)\n",
    "df2['city'] = location_split[0].str.strip().str.lower()\n",
    "df2['country'] = location_split[1].str.strip() if len(location_split.columns) > 1 else None\n",
    "\n",
    "df2 = df2[df2['city'].isin(allowed_cities)]\n",
    "df2.dropna(subset=['city'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70de6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Map country to city for missing city values\n",
    "allowed_cities = ['amsterdam', 'athens', 'barcelona', 'berlin', 'budapest', 'vienna', 'lisbon', 'london', 'paris', 'rome']\n",
    "\n",
    "country_to_city = {\n",
    "    'netherlands': 'amsterdam',\n",
    "    'greece': 'athens',\n",
    "    'spain': 'barcelona',\n",
    "    'germany': 'berlin',\n",
    "    'hungary': 'budapest',\n",
    "    'austria': 'vienna',\n",
    "    'portugal': 'lisbon',\n",
    "    'united kingdom': 'london',\n",
    "    'france': 'paris',\n",
    "    'italy': 'rome'\n",
    "}\n",
    "\n",
    "location_split = df2['host_location'].str.split(',', n=1, expand=True)\n",
    "temp_city = location_split[0].str.strip().str.lower()\n",
    "temp_country = location_split[1].str.strip().str.lower() if len(location_split.columns) > 1 else pd.Series([None] * len(df2))\n",
    "\n",
    "def finalize_location(row_idx):\n",
    "    city = temp_city.iloc[row_idx]\n",
    "    country = temp_country.iloc[row_idx]\n",
    "    \n",
    "    if city in allowed_cities:\n",
    "        return city, country\n",
    "    elif country in country_to_city:\n",
    "        return country_to_city[country], country\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "df2[['city', 'country']] = [finalize_location(i) for i in range(len(df2))]\n",
    "df2.dropna(subset=['city'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d370bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining rows: 196304\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove unwanted values\n",
    "unwanted = ['ga', 'in', 'canada', 'ny']\n",
    "df2 = df2[~df2['city'].str.lower().isin(unwanted)]\n",
    "df2 = df2[~df2['country'].str.lower().fillna('').isin(unwanted)]\n",
    "\n",
    "print(f\"Total remaining rows: {len(df2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a5d052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime and create weekend indicator\n",
    "df2['calendar_last_scraped'] = pd.to_datetime(df2['calendar_last_scraped'], format='mixed', dayfirst=False)\n",
    "df2['day_type'] = df2['calendar_last_scraped'].dt.dayofweek.apply(lambda x: 'weekend' if x >= 5 else 'weekday')\n",
    "df2['is_weekend'] = df2['day_type'].apply(lambda x: 1 if x == 'weekend' else 0)\n",
    "df2.drop(columns=['calendar_last_scraped'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6c27abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract amenities features\n",
    "target_amenities = ['wifi', 'kitchen', 'air_conditioning', 'parking', 'tv', 'heating']\n",
    "\n",
    "for amenity in target_amenities:\n",
    "    search_term = amenity.replace('_', ' ')\n",
    "    df2[amenity] = df2['amenities'].str.contains(search_term, case=False, na=False).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "672cce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Drop original amenities column\n",
    "df2.drop(columns=['amenities'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d92a3f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 shape: (196304, 32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"df2 shape: {df2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e6072fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns: 30\n",
      "Only in df: set()\n",
      "Only in df2: {'host_listings_count', 'host_location'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compare columns between dataframes\n",
    "common_cols = set(df.columns).intersection(set(df2.columns))\n",
    "only_in_df = set(df.columns) - set(df2.columns)\n",
    "only_in_df2 = set(df2.columns) - set(df.columns)\n",
    "\n",
    "print(f\"Common columns: {len(common_cols)}\")\n",
    "print(f\"Only in df: {only_in_df}\")\n",
    "print(f\"Only in df2: {only_in_df2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d5f33e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 251011\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge dataframes\n",
    "final_df = pd.concat([df, df2], ignore_index=True)\n",
    "print(f\"Total rows: {len(final_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf840049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (251011, 32)\n",
      "Total null values: 1597930\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check data types and nulls\n",
    "print(f\"Shape: {final_df.shape}\")\n",
    "print(f\"Total null values: {final_df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80143e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing 51707 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputing: 100%|██████████| 52/52 [00:12<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining nulls: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Impute amenities features using KNN\n",
    "amenities_cols = ['wifi', 'kitchen', 'air_conditioning', 'parking', 'tv', 'heating']\n",
    "\n",
    "for col in amenities_cols:\n",
    "    final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
    "\n",
    "core_cols = ['realSum', 'person_capacity', 'bedrooms', 'longitude', 'latitude']\n",
    "all_num_cols = list(set(core_cols + amenities_cols))\n",
    "\n",
    "df_with_nulls = final_df[final_df[amenities_cols].isnull().any(axis=1)].copy()\n",
    "df_no_nulls = final_df[final_df[amenities_cols].notnull().all(axis=1)].copy()\n",
    "\n",
    "if len(df_with_nulls) == 0:\n",
    "    print(\"No nulls found in amenities columns\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_no_nulls[all_num_cols])\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "    batch_size = 1000\n",
    "    imputed_list = []\n",
    "    \n",
    "    print(f\"Imputing {len(df_with_nulls)} rows...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(df_with_nulls), batch_size), desc=\"Imputing\"):\n",
    "        current_batch = df_with_nulls.iloc[i : i + batch_size][all_num_cols].copy()\n",
    "        reference = df_no_nulls[all_num_cols].sample(n=1000, random_state=42)\n",
    "        combined = pd.concat([current_batch, reference])\n",
    "        \n",
    "        combined_scaled = scaler.transform(combined)\n",
    "        imputed_scaled = imputer.fit_transform(combined_scaled)\n",
    "        imputed_unscaled = scaler.inverse_transform(imputed_scaled)\n",
    "        imputed_batch = imputed_unscaled[:len(current_batch)]\n",
    "        imputed_list.append(imputed_batch)\n",
    "    \n",
    "    imputed_final_array = np.vstack(imputed_list)\n",
    "    df_with_nulls[all_num_cols] = imputed_final_array\n",
    "    \n",
    "    for c in amenities_cols:\n",
    "        df_with_nulls[c] = (df_with_nulls[c] >= 0.5).astype(int)\n",
    "    \n",
    "    final_df = pd.concat([df_with_nulls, df_no_nulls]).sort_index()\n",
    "    \n",
    "    print(f\"Remaining nulls: {final_df[amenities_cols].isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47882bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wifi                0\n",
      "kitchen             0\n",
      "air_conditioning    0\n",
      "parking             0\n",
      "tv                  0\n",
      "heating             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_df[amenities_cols].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91f27e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 251011\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows: {len(final_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "807e06cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing 196304 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputing: 100%|██████████| 99/99 [01:05<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining nulls: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Impute distance features using KNN\n",
    "dist_cols = ['dist', 'metro_dist', 'attr_index', 'attr_index_norm', 'rest_index', 'rest_index_norm']\n",
    "\n",
    "for col in dist_cols:\n",
    "    final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
    "\n",
    "core_cols = ['realSum', 'longitude', 'latitude', 'person_capacity']\n",
    "all_num_cols = list(set(core_cols + dist_cols))\n",
    "\n",
    "df_with_nulls = final_df[final_df[dist_cols].isnull().any(axis=1)].copy()\n",
    "df_no_nulls = final_df[final_df[dist_cols].notnull().all(axis=1)].copy()\n",
    "\n",
    "if len(df_with_nulls) == 0:\n",
    "    print(\"No nulls in distance columns\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_no_nulls[all_num_cols])\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "    batch_size = 2000\n",
    "    imputed_list = []\n",
    "    \n",
    "    print(f\"Imputing {len(df_with_nulls)} rows...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(df_with_nulls), batch_size), desc=\"Imputing\"):\n",
    "        current_batch = df_with_nulls.iloc[i : i + batch_size][all_num_cols].copy()\n",
    "        reference = df_no_nulls[all_num_cols].sample(n=1500, random_state=42)\n",
    "        combined = pd.concat([current_batch, reference])\n",
    "        \n",
    "        combined_scaled = scaler.transform(combined)\n",
    "        imputed_scaled = imputer.fit_transform(combined_scaled)\n",
    "        imputed_unscaled = scaler.inverse_transform(imputed_scaled)\n",
    "        imputed_list.append(imputed_unscaled[:len(current_batch)])\n",
    "    \n",
    "    imputed_final_array = np.vstack(imputed_list)\n",
    "    df_with_nulls[all_num_cols] = imputed_final_array\n",
    "    \n",
    "    final_df = pd.concat([df_with_nulls, df_no_nulls]).sort_index()\n",
    "    \n",
    "    print(f\"Remaining nulls: {final_df[dist_cols].isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4e8bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 251011\n",
      "Total nulls: 109864\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows: {len(final_df)}\")\n",
    "print(f\"Total nulls: {final_df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db752ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nulls in bedrooms/beds columns\n",
      "Total nulls remaining: 109414\n",
      "Data saved successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Impute bedrooms and beds using KNN\n",
    "target_cols = ['bedrooms', 'beds']\n",
    "helper_cols = ['person_capacity', 'realSum', 'longitude', 'latitude']\n",
    "all_cols = target_cols + helper_cols\n",
    "\n",
    "df_with_nulls = final_df[final_df[target_cols].isnull().any(axis=1)].copy()\n",
    "df_no_nulls = final_df[final_df[target_cols].notnull().all(axis=1)].copy()\n",
    "\n",
    "if len(df_with_nulls) > 0:\n",
    "    print(f\"Imputing {len(df_with_nulls)} rows...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_no_nulls[all_cols])\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "    imputed_list = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in tqdm(range(0, len(df_with_nulls), batch_size), desc=\"Imputing\"):\n",
    "        batch = df_with_nulls.iloc[i : i + batch_size][all_cols].copy()\n",
    "        reference = df_no_nulls[all_cols].sample(n=2000, random_state=42)\n",
    "        combined = pd.concat([batch, reference])\n",
    "        \n",
    "        combined_scaled = scaler.transform(combined)\n",
    "        imputed_scaled = imputer.fit_transform(combined_scaled)\n",
    "        imputed_unscaled = scaler.inverse_transform(imputed_scaled)\n",
    "        imputed_list.append(imputed_unscaled[:len(batch)])\n",
    "    \n",
    "    df_with_nulls[all_cols] = np.vstack(imputed_list)\n",
    "    df_with_nulls[target_cols] = df_with_nulls[target_cols].round().astype(int)\n",
    "    \n",
    "    final_df = pd.concat([df_with_nulls, df_no_nulls]).sort_index()\n",
    "else:\n",
    "    print(\"No nulls in bedrooms/beds columns\")\n",
    "\n",
    "to_int_final = ['bedrooms', 'beds', 'wifi', 'kitchen', 'air_conditioning', 'parking', 'tv', 'heating']\n",
    "final_df[to_int_final] = final_df[to_int_final].astype(int)\n",
    "\n",
    "print(f\"Total nulls remaining: {final_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Save final output — outputs to the app data folder\n",
    "output_path = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"..\", \"app\", \"data\", \"airbnb_listings_clean.csv\")\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(\"Data saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b107d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (251011, 32)\n",
      "Total nulls: 109414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_14884\\286934636.py:5: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Verify final file\n",
    "file_path = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"..\", \"app\", \"data\", \"airbnb_listings_clean.csv\")\n",
    "final_df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Shape: {final_df.shape}\")\n",
    "print(f\"Total nulls: {final_df.isnull().sum().sum()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
